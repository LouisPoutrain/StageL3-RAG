import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# --- Charger les chunks (texte + m√©tadonn√©es) ---
with open("chunk_metadata.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

# --- Charger l‚Äôindex FAISS ---
index = faiss.read_index("article_index.faiss")

# --- Charger SentenceTransformer ---
encoder = SentenceTransformer("all-MiniLM-L6-v2")

# --- Initialiser le LLM local avec plus de contexte ---
llm = Llama(model_path="models/mistral-7b-instruct-v0.1.Q2_K.gguf", n_ctx=4096)

# --- Reformulation de la requ√™te utilisateur ---
def rewrite_query(question):
    prompt = f"""Tu es un assistant d'indexation scientifique. Ta t√¢che est de transformer une question en une phrase **descriptive et litt√©rale**, en t'appuyant uniquement sur les mots-cl√©s pr√©sents dans la question.

Ta reformulation doit respecter ces r√®gles :
- Utilise uniquement les informations pr√©sentes dans la question (aucune interpr√©tation, aucune inf√©rence).
- Ne change pas de sujet, ne g√©n√©ralise pas, ne compl√®te pas la question.
- Si certains mots sont ambigus, conserve-les tels quels.

Le but est de produire une phrase simple, factuelle.

Question : {question}
Phrase reformul√©e :"""

    result = llm(prompt, max_tokens=120, temperature=0.4)
    return result["choices"][0]["text"].strip()


# --- Recherche contextuelle dans l‚Äôindex FAISS ---
def search_query(query, k=5):
    query_embedding = encoder.encode([query])
    distances, indices = index.search(np.array(query_embedding), k)
    return [chunks[i] for i in indices[0]]

# --- G√©n√©ration avec s√©curit√© contre r√©ponses incompl√®tes ---
def safe_generate(prompt, llm, max_tokens=1024):
    full_response = ""
    for _ in range(3):  # autorise jusqu'√† 3 reprises
        result = llm(prompt + full_response, max_tokens=max_tokens, temperature=0.7)
        new_text = result["choices"][0]["text"]
        full_response += new_text.strip()
        if full_response.endswith((".", "!", "?", "‚Ä¶")):
            break
    return full_response.strip()

# --- G√©n√©ration finale avec le contexte ---
def generate_answer_local(question, retrieved_chunks):
    context = "\n\n".join([
        f"[{c['section']}]\n{c['text'][:500]}..."  # tronquer pour √©viter surcharge
        for c in retrieved_chunks
    ])

    prompt = f"""Tu es un assistant scientifique rigoureux.

R√©ponds √† la question suivante en t'appuyant **uniquement** sur les extraits de texte ci-dessous. Ta r√©ponse doit √™tre pr√©cise, factuelle et structur√©e. Si la r√©ponse ne peut pas √™tre d√©duite des extraits, indique-le explicitement.

D√©finition de r√©f√©rence √† utiliser :
"Selon Taberlet et al. (1999), l'√©chantillonnage non-invasif est une m√©thode de collecte de mat√©riel g√©n√©tique d'un organisme sans recours √† des techniques invasives comme l‚Äôanesth√©sie, la perforation de la peau, la destruction de tissus, ou tout acte susceptible d'alt√©rer le comportement ou la survie de l‚Äôanimal."

Extraits :
{context}

Question : {question}

R√©ponse :"""

    return safe_generate(prompt, llm, max_tokens=1024)

# --- Pipeline RAG complet ---
if __name__ == "__main__":
    question = input("La question : ").strip()

    # √âtape 1 : reformulation
    print("\nüîß Reformulation de la question....")
    rewritten = rewrite_query(question)
    print("üîç Phrase utilis√©e pour l'index :", rewritten)

    # √âtape 2 : recherche contextuelle
    retrieved = search_query(rewritten, k=6)

    # √âtape 3 : g√©n√©ration de r√©ponse
    answer = generate_answer_local(question, retrieved)

    print("\n Question :", question)
    print("\nR√©ponse g√©n√©r√©e :\n", answer)
