import argparse
import sys
import os
import pandas as pd
from rag_system import RAGSystem, RefineRAGSystem, parse_json, RAGNonInvasiveDetection
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
from time import time
import json
from dotenv import load_dotenv
import nltk

DATAFRAME_COLUMNS = [
    'Filename', 'Protocole', 'echantillon', 'Impacts potentiels', 'Extrait pertinent', 
    '√âvaluation d\'invasivit√©', 'Justification d\'invasivit√©', 'P√©ch√©s identifi√©s', 'Taux de confiance', 
    'annonce_invasivite', 'justification_annonce_invasivite'
]

def process_file(api_key, api_url, json_path, definition, question, method, top_k, output_dir):
    """
    Traite un unique fichier JSON pour en extraire les protocoles, les √©valuer
    et analyser leur pr√©sentation.

    Args:
        api_key (str): Cl√© d'API pour le service LLM.
        api_url (str): URL de l'API LLM.
        json_path (str): Chemin vers le fichier JSON √† traiter.
        definition (str): D√©finition de l'invasivit√© √† utiliser.
        question (str): Question √† poser au RAG pour l'extraction de protocole.
        method (str): M√©thode d'analyse √† utiliser ('refine', 'both', ...).  (Obselete)
        top_k (int): Nombre de documents pertinents √† r√©cup√©rer.
        output_dir (str): R√©pertoire o√π sauvegarder les logs.

    Returns:
        tuple: Un tuple contenant:
            - df_result (pd.DataFrame): DataFrame avec les r√©sultats de l'analyse.
            - impacts_energy (list): Liste des impacts √©nerg√©tiques de l'analyse principale.
            - impacts_co2 (list): Liste des impacts CO2 de l'analyse principale.
            - impacts_energy_non_invasive (list): Liste des impacts √©nerg√©tiques de l'analyse d'invasivit√©.
            - impacts_co2_non_invasive (list): Liste des impacts CO2 de l'analyse d'invasivit√©.
    """
    base_name = os.path.splitext(os.path.basename(json_path))[0]
    log_path = os.path.join(output_dir, f"{base_name}.txt")

    df_result = pd.DataFrame(columns=DATAFRAME_COLUMNS)

    with open(log_path, "w", encoding="utf-8") as f:
        # Redirection de la sortie standard (print) vers un fichier de log propre √† chaque fichier trait√©.
        original_stdout = sys.stdout
        sys.stdout = f

        try:
            print(f"Traitement du fichier : {json_path}")
            start_time = time()
            
            rag_system = RefineRAGSystem(api_key=api_key, api_url=api_url) 
            rag_system.index_from_json(json_path) # Indexation des documents
            rag_system_non_invasive_detection = RAGNonInvasiveDetection(api_key=api_key, api_url=api_url) # Pour l'analyse sur l'annonce de l'invasivit√©

            if method in ["refine", "both"]:
                print("\n--- M√©thode avec Refine ---")
                refine_answer = rag_system.refine_analysis(question, definition, top_k=top_k, title=base_name) # Lancement de l'analyse via la m√©thode Refine
                
                json_final = refine_answer
                resultats = parse_json(json_final, base_name)
                
                if not resultats.empty:
                    
                    print("\n--- M√©thode avec Refine et detect_non_invasive_level ---")
                    # Pour chaque protocole dans les r√©sultats, on r√©cup√®re les infos utiles pour la suite
                    for idx, row in resultats.iterrows():
                        protocole = {
                            "nom": row["Protocole"],
                            "description": row["Extrait pertinent"],
                            "evaluation": row["√âvaluation d'invasivit√©"]
                        }

                        # Partie pour l'analyse sur l'anonce de l'invasivit√© lorsqu'un protocole est detect√© comme invasif
                        if protocole['evaluation'] == 'Invasif' or protocole['evaluation'] == 'Invasif - Territory marking':
                            try:
                                # Pour les protocoles jug√©s invasifs, une seconde analyse est lanc√©e pour v√©rifier
                                # si l'article pr√©sente lui-m√™me le protocole comme "non invasif".
                                invasive_json_path = os.path.join("output_invasive_detection", f"{base_name}.json")
                                # On indexe le fichier
                                rag_system_non_invasive_detection.index_from_json(invasive_json_path)
                                # Le titre de l'article est important pour la technique HyDE (Hypothetical Document Embeddings)
                                title = base_name
                                # Charge le fichier JSON contenant les chunks pour l'annonce de l'invasivit√© (pas le m√™me que pour la d√©t√©ctions des protocoles car contient plus de sections (Intro, abstract...))
                                with open(invasive_json_path, "r", encoding="utf-8") as f:
                                    invasive_data = json.load(f)
                                for section in invasive_data:
                                    # r√©cup√®re le titre dans la section titre du JSON (pour le fournir √† HyDE)
                                    if section["section"] == "title":
                                        title = section["text"]
                                        break
                                
                                # Une question sp√©cifique est formul√©e pour √©valuer comment le protocole est pr√©sent√© dans l'article.
                                question = f"{protocole['nom'].capitalize()} constitue-t-il une m√©thode non invasive ou minimalement invasive pour l'obtention d'√©chantillons g√©n√©tiques chez les animaux √©tudi√©s ?"
                                non_invasive_level = rag_system_non_invasive_detection.answer_question(
                                    question=question,
                                    protocole=str(protocole),
                                    top_k=8,
                                    title=title
                                )
                                
                                print(f"\nAnalyse du niveau non invasif pour le protocole {protocole['nom']}:")
                                print(non_invasive_level)
                                non_invasive_level = parse_json(non_invasive_level, base_name, invasive_detection=True) # R√©cup√®re le JSON extrait de la r√©ponse du LLM
                                print(f"non_invasive_level: {non_invasive_level}")

                                # Ins√©rer les donn√©es dans le DataFrame
                                resultats.at[idx, 'annonce_invasivite'] = non_invasive_level[0]['annonce_invasivite']
                                resultats.at[idx, 'justification_annonce_invasivite'] = non_invasive_level[0]['justification_annonce_invasivite']
                            except Exception as e:
                                print(f"Erreur lors de l'analyse du niveau non invasif pour le protocole {protocole['nom']}: {str(e)}")
                                resultats.at[idx, 'annonce_invasivite'] = f"Erreur: {str(e)}"
                                resultats.at[idx, 'justification_annonce_invasivite'] = "Erreur lors de l'analyse"
                    print("--------------------------------")
                    
                    # V√©rification et nettoyage des DataFrames avant la concat√©nation
                    if df_result.empty:
                        df_result = resultats.copy()
                    else:
                        # S'assurer que les types de colonnes sont compatibles
                        for col in df_result.columns:
                            if col in resultats.columns:
                                df_result[col] = df_result[col].astype(resultats[col].dtype)
                        df_result = pd.concat([df_result, resultats], ignore_index=True)
                    print(f"\nTraitement termin√© avec succ√®s pour le fichier : {json_path}")
                else:
                    print(f"\nAucun r√©sultat g√©n√©r√© pour le fichier : {json_path}")

        except Exception as e:
            print(f"Erreur lors du traitement du fichier {json_path}: {str(e)}")
            import traceback
            print(traceback.format_exc())
        finally:
            # Retour √† la sortie original
            sys.stdout = original_stdout
            print(f"Traitement termin√© avec succ√®s pour : {os.path.basename(json_path)} en {time() - start_time:.2f} secondes")

    return df_result, rag_system.impacts_energy, rag_system.impacts_co2, rag_system_non_invasive_detection.impacts_energy, rag_system_non_invasive_detection.impacts_co2

# Wrapper pour passer plusieurs arguments √† process_file via ProcessPoolExecutor.
def process_file_wrapper(args):
    """
    Wrapper pour la fonction `process_file`.

    Cette fonction est n√©cessaire pour utiliser `ProcessPoolExecutor.submit` qui
    ne peut passer qu'un seul argument √† la fonction cible. Elle prend un tuple
    d'arguments et le d√©paquette pour appeler `process_file`.

    Args:
        args (tuple): Un tuple contenant tous les arguments pour `process_file`.

    Returns:
        Le r√©sultat de l'appel √† `process_file`.
    """
    return process_file(*args)

def main():
    """
    Fonction principale du script.

    Parse les arguments de la ligne de commande, pr√©pare les t√¢ches pour chaque
    fichier JSON √† analyser, et lance le traitement en parall√®le.
    Agr√®ge ensuite les r√©sultats et les sauvegarde dans un fichier CSV.
    """
    # Charge les variables d'environnement √† partir d'un fichier .env.
    load_dotenv()
    
    # S'assure que les packages NLTK n√©cessaires sont t√©l√©charg√©s une seule fois.
    try:
        nltk.data.find('tokenizers/punkt')
    except nltk.downloader.DownloadError:
        nltk.download('punkt')
    
    parser = argparse.ArgumentParser(description="Syst√®me RAG pour l'analyse de documents scientifiques")
    parser.add_argument("--api_key", help="Cl√© API pour le LLM")
    parser.add_argument("--api_url", help="URL de l'API LLM")
    parser.add_argument("--json_file", default="output/A novel molecular method for noninvasive sex identification of order Carnivora.json")
    parser.add_argument("--definition", default="Selon Taberlet et al. (1999), l'√©chantillonnage d'ADN non invasif d√©signe toute m√©thode permettant d'obtenir du mat√©riel g√©n√©tique sans avoir √† capturer, blesser, ni perturber significativement l'animal. Cela inclut, par exemple, l'analyse d'√©chantillons laiss√©s dans l'environnement comme les poils, les plumes, les f√®ces, l'urine, ou encore la salive.")
    parser.add_argument("--question", default="Dans l'article, peux-tu me donner tous les protocoles d'√©chantillonnage d'ADN et s'ils sont consid√©r√©s comme invasifs ou non selon la d√©finition de Taberlet ?")
    parser.add_argument("--method", choices=["refine", "standard", "both", "detect_non_invasive_level"], default="refine")
    parser.add_argument("--top_k", type=int, default=8)
    parser.add_argument("--output_dir", default="R√©sultats") # Dossier pour retrouver les r√©sultats
    parser.add_argument("--input_dir", default="output") # Dossier o√π les fichiers JSON sont stock√©s
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--max_workers", type=int, default=4)
    
    args = parser.parse_args()

    # R√©cup√©ration de la cl√© API et de l'URL depuis les arguments ou les variables d'environnement.
    api_key = args.api_key or os.getenv("LLM_API_KEY")
    api_url = args.api_url or os.getenv("LLM_API_URL", "http://gpu1.pedagogie.sandbox.univ-tours.fr:32800/api/chat/completions")

    if not api_key:
        # Si la cl√© API n'est pas trouv√©e, le programme s'arr√™te avec un message d'erreur.
        print("Erreur : La cl√© API n'est pas fournie.", file=sys.stderr)
        print("Veuillez la passer avec --api_key ou d√©finir la variable d'environnement LLM_API_KEY.", file=sys.stderr)
        sys.exit(1)

    os.makedirs(args.output_dir, exist_ok=True)
    json_files = [f for f in os.listdir(args.input_dir) if f.endswith(".json")]
    
    df_total = pd.DataFrame(columns=DATAFRAME_COLUMNS)

    # Variables pour suivre les impacts totaux
    total_energy = []
    total_co2 = []
    total_energy_non_invasive = []
    total_co2_non_invasive = []

    # Cr√©ation des t√¢ches par lots
    tasks = []
    for i in range(0, len(json_files), args.batch_size):
        batch = json_files[i:i + args.batch_size]
        for filename in batch:
            json_path = os.path.join(args.input_dir, filename)
            tasks.append((
                api_key,
                api_url,
                json_path,
                args.definition,
                args.question,
                args.method,
                args.top_k,
                args.output_dir
            ))

    print(f"Type de la cl√© dans tasks : {type(tasks[0][0])}")

    results_list = []
    # Utilisation de ProcessPoolExecutor pour traiter les fichiers en parall√®le.
    # Chaque fichier est trait√© dans un processus distinct
    with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
        futures = [executor.submit(process_file_wrapper, task) for task in tasks]

        for future in tqdm(as_completed(futures), total=len(futures), desc="Fichiers trait√©s"):
            try:
                df_result, impacts_energy, impacts_co2, impacts_energy_non_invasive, impacts_co2_non_invasive = future.result()
                if not df_result.empty:
                    results_list.append(df_result)

                # Ajouter les impacts aux totaux
                total_energy.extend(impacts_energy)
                total_co2.extend(impacts_co2)
                total_energy_non_invasive.extend(impacts_energy_non_invasive)
                total_co2_non_invasive.extend(impacts_co2_non_invasive)

                # Sauvegarde interm√©diaire des r√©sultats cumul√©s.
                if results_list:
                    pd.concat(results_list, ignore_index=True).to_csv(os.path.join(args.output_dir, "Protocoles_intermediaire.csv"), index=False)

            except Exception as e:
                print(f"Erreur dans un traitement parall√®le : {e}")

    if results_list:
        df_total = pd.concat(results_list, ignore_index=True)

    # Sauvegarde finale
    if not df_total.empty:
        df_total.to_csv(os.path.join(args.output_dir, "Protocoles.csv"), index=False)
        print("Traitement termin√©. R√©sultats sauvegard√©s dans 'Protocoles.csv'.")
        
        # Afficher les impacts totaux
        total_energy_all = sum(total_energy) + sum(total_energy_non_invasive)
        total_co2_all = sum(total_co2) + sum(total_co2_non_invasive)
        print(f"\nüå± √ânergie totale consomm√©e : {total_energy_all:.4f} kWh")
        print(f"üåç CO‚ÇÇ total √©mis : {total_co2_all:.2f} g")
        
        return df_total
    else:
        print("Attention: Aucun r√©sultat n'a √©t√© g√©n√©r√©.")
        return None

if __name__ == "__main__":
    main()



 
                            

            
